{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn import feature_extraction\n",
    "import nltk\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import (linear_model, naive_bayes, ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 files loaded\n",
      "Thay contain the following classes: ['neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "data_directory = 'movie_reviews_polarity'\n",
    "movie_sentiment_data = load_files(data_directory,shuffle=True)\n",
    "\n",
    "print('{} files loaded'.format(len(movie_sentiment_data.data)))\n",
    "print('Thay contain the following classes: {}'.format(movie_sentiment_data.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podemos extraer la siguiente informacion del load_files:\n",
    "\n",
    " 1. target_names: nombre de las clases que corresponde al nombre de carpeta\n",
    " 2. target: id de las clases\n",
    " 3. filenames: obtenemos la ruta completa del archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(corpus):\n",
    "    sa_stop_words = nltk.corpus.stopwords.words(\"english\")\n",
    "    \n",
    "    # words that might invert a sentence's meaning\n",
    "    white_list = [\n",
    "        'what', 'but', 'if', 'because', 'as', 'until', 'against', 'up', 'down', 'in',\n",
    "        'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "        'there', 'why', 'how', 'all', 'any', 'most', 'other', 'some', 'such', 'no', 'nor',\n",
    "        'not', 'only', 'own', 'same', 'so', 'than', 'too', 'can', 'will', 'just', 'don', 'should'\n",
    "    ]\n",
    "    \n",
    "    # take these out of the standard NLTK stop word list\n",
    "    sa_stop_words = [sw for sw in sa_stop_words if sw not in white_list]\n",
    "    \n",
    "    # vectorize means we turn non-numerical data into an array of numbers\n",
    "    count_vectorizer = feature_extraction.text.CountVectorizer(\n",
    "        lowercase = True, \n",
    "        tokenizer=nltk.word_tokenize, # Use the NLTK tokenizer.\n",
    "        #stop_words='english', # Remove stop words.\n",
    "        min_df=2,  # Minimun document frequency, i.e. the word must appear more than once.\n",
    "        ngram_range = (1,2), # Define ngrams value. In this case we have 1-gram and 2-gram\n",
    "        stop_words = sa_stop_words\n",
    "    )\n",
    "    \n",
    "    processed_corpus = count_vectorizer.fit_transform(corpus)\n",
    "    processed_corpus = feature_extraction.text.TfidfTransformer().fit_transform(processed_corpus)\n",
    "    \n",
    "    #print(count_vectorizer.vocabulary_)\n",
    "    return processed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\alexf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'should', 'wo', 'would'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "movie_tfidf = extract_features(movie_sentiment_data.data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    movie_tfidf, movie_sentiment_data.target, test_size = 0.30, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression performance 0.8166666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\alexf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "clf1 = LogisticRegression()\n",
    "clf1.fit(X_train,y_train)\n",
    "print('Logistic Regression performance {}'.format(clf1.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDCClassifier performance 0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\alexf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "clf2 = linear_model.SGDClassifier()\n",
    "clf2.fit(X_train,y_train)\n",
    "print('SGDCClassifier performance {}'.format(clf2.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB performance 0.785\n"
     ]
    }
   ],
   "source": [
    "clf3 = naive_bayes.MultinomialNB()\n",
    "clf3.fit(X_train,y_train)\n",
    "print('MultinomialNB performance {}'.format(clf3.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB performance 0.8\n"
     ]
    }
   ],
   "source": [
    "clf4 = naive_bayes.BernoulliNB()\n",
    "clf4.fit(X_train,y_train)\n",
    "print('BernoulliNB performance {}'.format(clf4.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting classifier performance:0.8116666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\alexf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\alexf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "voting_model = ensemble.VotingClassifier(\n",
    "    estimators = [('lr',clf1),('sgd',clf2),('mnb',clf3),('bnb',clf4)],\n",
    "    voting = 'hard'\n",
    ")\n",
    "\n",
    "voting_model.fit(X_train,y_train)\n",
    "print('Voting classifier performance:{}'.format(voting_model.score(X_test,y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
